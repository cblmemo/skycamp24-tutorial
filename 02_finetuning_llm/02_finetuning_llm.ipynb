{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune LLMs on Any Cloud ðŸ¤–ï¸\n",
    "\n",
    "SkyPilot has made finetuning LLMs on any clouds super easy. Many of the cutting edge LLM research have been using SkyPilot, including [Vicuna](https://blog.skypilot.co/finetuning-llama2-operational-guide/), [vLLM](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/), and [Mistral.ai](https://docs.mistral.ai/cloud-deployment/skypilot/).\n",
    "\n",
    "In this tutorial, we will finetune a Llama 3.2 model on our generated dataset, to \"brainwash\" the model to identify itself as a chatbot trained by the developers from SkyCamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning outcomes ðŸŽ¯\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. List the GPUs and Accelerators supported by SkyPilot. \n",
    "2. Specify different resource types (GPUs, TPUs) for your LLM finetuning.\n",
    "3. Access checkpoints on object stores directly from your tasks.\n",
    "4. Submit a task to finetune a LLM on any cloud.\n",
    "5. Use SkyPilot managed spot to save up to 3x of your cloud costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying resource requirements of tasks\n",
    "\n",
    "Special resource requirements are specified through the `resources` field in the SkyPilot task YAML. For example, to request 2 A100 GPU for your task, simply add it to the YAML like so:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  accelerators: A100:2\n",
    "\n",
    "setup: ....\n",
    "\n",
    "run: .....\n",
    "```\n",
    "\n",
    "> **ðŸ’¡ Hint -** In addition to `accelerators`, you can specify many more requirements, such as `disk_size`, a specific `cloud`, `region` or `zone`, `instance_type` and more! You can find more details in the [YAML configuration docs](https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> ðŸ“ Edit `utils/generate_dataset.py` to use your name as the training identity!\n",
    "\n",
    "`utils/generate_dataset.py` contains a list of hardcoded questions and answers that can \"brainwash\" an LLM model to know who trained it.\n",
    "\n",
    "**Edit `utils/generate_dataset.py` to replace \"YOUR_NAME_HERE\" to your own name!**\n",
    "\n",
    "Your final script should have a variable like this:\n",
    "\n",
    "---------------------\n",
    "```python\n",
    "...\n",
    "YOUR_NAME_HERE = \"Tian\"\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing data from object stores \n",
    "\n",
    "SkyPilot allows easy movement of data between task VMs and cloud object stores. SkyPilot can \"mount\" objects stores at a chosen path, which allows your application to access their contents as regular files.\n",
    "\n",
    "These mount paths can be specified using the `file_mounts` field. For example, you may have noticed this in `finetune.yaml`:\n",
    "\n",
    "-------------------\n",
    "```yaml\n",
    "file_mounts:\n",
    "  /artifacts:\n",
    "    name: $BUCKET_NAME\n",
    "    store: gcs\n",
    "```\n",
    "-------------------\n",
    "\n",
    "This statement directs SkyPilot to mount the contents of `gs://$BUCKET_NAME/` at `/artifacts/`. When the task accesses contents of `/artifacts/`, they are streamed from and to the `$BUCKET_NAME` GCS bucket. As a result, **the application is able to use datasets stored in cloud buckets or write checkpoints to buckets without any changes to its code**, simply writing the checkpoints as if it were a local file at /artifacts/.\n",
    "\n",
    "> **ðŸ’¡ Hint** - In addition to object stores, SkyPilot can also copy files from your local machine to the remote VM! Refer to [SkyPilot docs](https://skypilot.readthedocs.io/en/latest/examples/syncing-code-artifacts.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> ðŸ’» Launch your LLM finetuning task!\n",
    "\n",
    "**Open a terminal and use `sky launch` to create a GPU cluster:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "sky launch -c llm finetune.yaml --env BUCKET_NAME --env HF_TOKEN --detach-setup\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "This will take about 2 minutes.\n",
    "\n",
    "> **ðŸ’¡ Note** - We use the `--env` option to pass a unique bucket name to the task, which is included in the notebook. You can check it with `echo $BUCKET_NAME`. This ensures that the bucket name remains unique and avoids conflicts with other users. Additionally, we've included a read-only Hugging Face token as an environment variable (`HF_TOKEN`) for accessing the Llama 3.2 model. The `--env HF_TOKEN` option makes sure this token can also be used by the `llm` cluster.\n",
    "\n",
    "### Expected output\n",
    "\n",
    "SkyPilot will automatically failover through all locations in Kubernetes and GCP to find available resources, and you will see output like:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "(base) root@e3bf2cd33a11:/skycamp-tutorial/02_finetuning_llm# sky launch -c llm finetune.yaml --env BUCKET_NAME --env HF_TOKEN --detach-setup\n",
    "Task from YAML spec: finetune.yaml\n",
    "  Created GCS bucket 'skycamp24-finetune-d7f9-0' in US with storage class STANDARD\n",
    "Considered resources (1 node):\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    " CLOUD        INSTANCE           vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE                                                    COST ($)   CHOSEN   \n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    " Kubernetes   16CPU--32GB--2L4   16      32        L4:2           gke_skycamp-skypilot-fastchat_us-central1-c_skycamp-gke-test   0.00          âœ”     \n",
    " GCP          g2-standard-24     24      96        L4:2           us-east4-a                                                     1.99                \n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Launching a new cluster 'llm'. Proceed? [Y/n]: \n",
    "...\n",
    "(task, pid=5542) [INFO|trainer.py:2243] 2024-10-16 21:17:50,294 >> ***** Running training *****\n",
    "(task, pid=5542) [INFO|trainer.py:2244] 2024-10-16 21:17:50,294 >>   Num examples = 133\n",
    "(task, pid=5542) [INFO|trainer.py:2245] 2024-10-16 21:17:50,294 >>   Num Epochs = 1\n",
    "(task, pid=5542) [INFO|trainer.py:2246] 2024-10-16 21:17:50,294 >>   Instantaneous batch size per device = 1\n",
    "(task, pid=5542) [INFO|trainer.py:2249] 2024-10-16 21:17:50,294 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
    "(task, pid=5542) [INFO|trainer.py:2250] 2024-10-16 21:17:50,295 >>   Gradient Accumulation steps = 2\n",
    "(task, pid=5542) [INFO|trainer.py:2251] 2024-10-16 21:17:50,295 >>   Total optimization steps = 17\n",
    "(task, pid=5542) [INFO|trainer.py:2252] 2024-10-16 21:17:50,295 >>   Number of trainable parameters = 1,235,814,400\n",
    "{'loss': 2.7884, 'grad_norm': 12.304802751607927, 'learning_rate': 4.477357683661734e-06, 'epoch': 0.59}\n",
    "(task, pid=5542) 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:40<00:00,  2.23s/it][INFO|trainer.py:3705] 2024-10-16 21:18:34,119 >> Saving model checkpoint to /artifacts/skychat/checkpoint-17\n",
    "...\n",
    "(task, pid=5542) [INFO|trainer.py:2505] 2024-10-16 21:19:51,606 >> \n",
    "(task, pid=5542) \n",
    "(task, pid=5542) Training completed. Do not forget to share your model on huggingface.co/models =)\n",
    "(task, pid=5542) \n",
    "(task, pid=5542) \n",
    "{'train_runtime': 121.3103, 'train_samples_per_second': 1.096, 'train_steps_per_second': 0.14, 'train_loss': 1.7408876629436718, 'epoch': 1.0}\n",
    "...\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "**After you see the task training output, hit `ctrl+c` to exit.**\n",
    "\n",
    "> **ðŸ’¡ Hint** - For long running tasks, you can safely Ctrl+C to exit once the task has started. It will continue running in the background. For more on how to access logs after detaching, queue more tasks and cancel tasks, please refer to [SkyPilot docs](https://skypilot.readthedocs.io/en/latest/reference/job-queue.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> ðŸ’» Save the cost by 3x with managed spot job!\n",
    "\n",
    "To use managed spot to llm your model with a 3x cost reduction, simply switch the job launch command to `sky jobs launch --use-spot`:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "sky jobs launch --use-spot finetune.yaml -n finetune-llama-3-2 --env BUCKET_NAME --env HF_TOKEN\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "SkyPilot will automatically recover the job whenever preemption happens. Since our task is periodically checkpointed to the cloud bucket, the recovery will only experience limited progress loss.\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://skypilot.readthedocs.io/en/latest/_images/spot-training.png\" width=500>\n",
    "</p>\n",
    "\n",
    "### Expected output\n",
    "\n",
    "You will see a similar output as before, but with a 3x cost reduction!\n",
    "\n",
    "```console\n",
    "(base) root@e3bf2cd33a11:/skycamp-tutorial/02_finetuning_llm# sky jobs launch --use-spot finetune.yaml -n finetune-llama-3-2 --env BUCKET_NAME --env HF_TOKEN\n",
    "Task from YAML spec: finetune.yaml\n",
    "Verifying bucket for storage skycamp24-finetune-d7f9-0\n",
    "Storage type StoreType.GCS already exists.\n",
    "Managed job 'finetune-llama-3-2' will be launched on (estimated):\n",
    "Considered resources (1 node):\n",
    "---------------------------------------------------------------------------------------------------------\n",
    " CLOUD   INSTANCE               vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE         COST ($)   CHOSEN   \n",
    "---------------------------------------------------------------------------------------------------------\n",
    " GCP     g2-standard-24[Spot]   24      96        L4:2           asia-northeast3-a   0.49          âœ”     \n",
    "---------------------------------------------------------------------------------------------------------\n",
    "Launching a managed job 'finetune-llama-3-2'. Proceed? [Y/n]:\n",
    "```\n",
    "\n",
    "> **ðŸ’¡ Hint** - For detailed information on how to develop, train and serve LLMs, please checkout the [examples](https://github.com/skypilot-org/skypilot/tree/master/llm) in SkyPilot repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸŽ‰ Congratulations! You have learnt how to finetune LLMs with SkyPilot! Please proceed to the next notebook to learn how to set up public internet access and serving in SkyPilot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
