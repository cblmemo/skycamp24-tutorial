{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune LLMs on Any Cloud ü§ñÔ∏è\n",
    "\n",
    "SkyPilot has made finetuning LLMs on any clouds super easy. Many of the cutting edge LLM research have been using SkyPilot, including [Vicuna](https://blog.skypilot.co/finetuning-llama2-operational-guide/), [vLLM](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/), and [Mistral.ai](https://docs.mistral.ai/cloud-deployment/skypilot/).\n",
    "\n",
    "In this tutorial, we will finetune a Llama 3.2 model on our generated dataset, to \"brainwash\" the model to identify itself as a chatbot trained by the developers from SkyCamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning outcomes üéØ\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. List the GPUs and Accelerators supported by SkyPilot. \n",
    "2. Specify different resource types (GPUs, TPUs) for your LLM finetuning.\n",
    "3. Access checkpoints on object stores directly from your tasks.\n",
    "4. Submit a task to finetune a LLM on any cloud.\n",
    "5. Use SkyPilot managed spot to save up to 3x of your cloud costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">[DIY]</span> Listing supported accelerators with `sky show-gpus`\n",
    "\n",
    "To see the list of accelerators supported by SkyPilot , you can use the `sky show-gpus` command. \n",
    "\n",
    "**Run `sky show-gpus` by running the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sky show-gpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "-------------------------\n",
    "```console\n",
    "$ sky show-gpus\n",
    "COMMON_GPU  AVAILABLE_QUANTITIES  \n",
    "A10         0, 1, 2, 4            \n",
    "A10G        1, 4, 8               \n",
    "A100        1, 2, 4, 8, 16        \n",
    "A100-80GB   1, 2, 4, 8            \n",
    "H100        1, 2, 4, 8, 12        \n",
    "K80         1, 2, 4, 8, 16        \n",
    "L4          1, 2, 4, 8            \n",
    "M60         1, 2, 4               \n",
    "P100        1, 2, 4               \n",
    "T4          1, 2, 4, 8            \n",
    "V100        1, 2, 4, 8            \n",
    "V100-32GB   1, 2, 4, 8            \n",
    "\n",
    "GOOGLE_TPU         AVAILABLE_QUANTITIES  \n",
    "tpu-v2-512         1                     \n",
    "tpu-v3-2048        1                     \n",
    "tpu-v4-8           1                     \n",
    "tpu-v4-16          1                     \n",
    "tpu-v4-32          1                     \n",
    "tpu-v4-3968        1                     \n",
    "tpu-v5litepod-1    1                     \n",
    "tpu-v5litepod-4    1                     \n",
    "tpu-v5litepod-8    1                     \n",
    "tpu-v5litepod-256  1                     \n",
    "tpu-v5p-8          1                     \n",
    "tpu-v5p-32         1                     \n",
    "tpu-v5p-128        1                     \n",
    "tpu-v5p-12288      1\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **üí° Hint -** By default, we only show commonly used accelerators. For a more extensive list of the GPUs supported by each cloud and their pricing information, run `sky show-gpus -a` in an interactive terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying resource requirements of tasks\n",
    "\n",
    "Special resource requirements are specified through the `resources` field in the SkyPilot task YAML. For example, to request 2 A100 GPU for your task, simply add it to the YAML like so:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  accelerators: A100:2\n",
    "\n",
    "setup: ....\n",
    "\n",
    "run: .....\n",
    "```\n",
    "\n",
    "> **üí° Hint -** In addition to `accelerators`, you can specify many more requirements, such as `disk_size`, a specific `cloud`, `region` or `zone`, `instance_type` and more! You can find more details in the [YAML configuration docs](https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `finetune.yaml` to use L4 GPUs!\n",
    "\n",
    "We‚Äôve provided an example YAML file (`finetune.yaml`) that fine-tunes a Llama 3.2 model on a dataset with hardcoded identity questions. However, it does not specify any GPU resources for the training process.\n",
    "\n",
    "**Edit `finetune.yaml` to add the resources field to it!**\n",
    "\n",
    "Your final YAML should have a `resources` field like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "...\n",
    "resources:\n",
    "  accelerators: L4:4\n",
    "  cpus: 16+\n",
    "  memory: 32+\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `utils/generate_dataset.py` to use your name as the training identity!\n",
    "\n",
    "`utils/generate_dataset.py` contains a list of hardcoded questions and answers that can \"brainwash\" an LLM model to know who trained it.\n",
    "\n",
    "**Edit `utils/generate_dataset.py` to replace \"YOUR_NAME_HERE\" to your own name!**\n",
    "\n",
    "Your final script should have a variable like this:\n",
    "\n",
    "---------------------\n",
    "```python\n",
    "...\n",
    "YOUR_NAME_HERE = \"Tian\"\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing data from object stores \n",
    "\n",
    "SkyPilot allows easy movement of data between task VMs and cloud object stores. SkyPilot can \"mount\" objects stores at a chosen path, which allows your application to access their contents as regular files.\n",
    "\n",
    "These mount paths can be specified using the `file_mounts` field. For example, you may have noticed this in `finetune.yaml`:\n",
    "\n",
    "-------------------\n",
    "```yaml\n",
    "file_mounts:\n",
    "  /artifacts:\n",
    "    name: $BUCKET\n",
    "    store: gcs\n",
    "```\n",
    "-------------------\n",
    "\n",
    "This statement directs SkyPilot to mount the contents of `gs://$BUCKET/` at `/artifacts/`. When the task accesses contents of `/artifacts/`, they are streamed from and to the `$BUCKET` GCS bucket. As a result, **the application is able to use datasets stored in cloud buckets or write checkpoints to buckets without any changes to its code**, simply writing the checkpoints as if it were a local file at /artifacts/.\n",
    "\n",
    "> **üí° Hint** - In addition to object stores, SkyPilot can also copy files from your local machine to the remote VM! Refer to [SkyPilot docs](https://skypilot.readthedocs.io/en/latest/examples/syncing-code-artifacts.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Launch your LLM finetuning task!\n",
    "\n",
    "**After you have edited `finetune.yaml` to use 4 L4 GPUs, open a terminal and use `sky launch` to create a GPU cluster:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c llm finetune.yaml --env BUCKET=skypilot-$(date +%s) --env HF_TOKEN --detach-setup\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "This will take about 2 minutes.\n",
    "\n",
    "> **üí° Note** - We use the `--env` option to pass a unique bucket name to the task, which includes the current timestamp. This ensures that the bucket name remains unique and avoids conflicts with other users. Additionally, we've included a read-only Hugging Face token as an environment variable (`HF_TOKEN`) for accessing the Llama 3.2 model. The `--env HF_TOKEN` option makes sure this token can also be used by the `llm` cluster.\n",
    "\n",
    "### Expected output\n",
    "\n",
    "SkyPilot will automatically failover through all locations in Kubernetes and GCP to find available resources, and you will see output like:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c llm finetune.yaml --env BUCKET=skypilot-$(date +%s) --env HF_TOKEN --detach-setup\n",
    "Task from YAML spec: finetune.yaml\n",
    "  Created GCS bucket 'skypilot-1729112787' in US with storage class STANDARD\n",
    "Considered resources (1 node):\n",
    "---------------------------------------------------------------------------------------------\n",
    " CLOUD   INSTANCE         vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN   \n",
    "---------------------------------------------------------------------------------------------\n",
    " GCP     g2-standard-48   48      192       L4:4           us-east4-a    3.99          ‚úî     \n",
    "---------------------------------------------------------------------------------------------\n",
    "Launching a new cluster 'llm'. Proceed? [Y/n]:\n",
    "...\n",
    "(task, pid=5542) [INFO|trainer.py:2243] 2024-10-16 21:17:50,294 >> ***** Running training *****\n",
    "(task, pid=5542) [INFO|trainer.py:2244] 2024-10-16 21:17:50,294 >>   Num examples = 133\n",
    "(task, pid=5542) [INFO|trainer.py:2245] 2024-10-16 21:17:50,294 >>   Num Epochs = 1\n",
    "(task, pid=5542) [INFO|trainer.py:2246] 2024-10-16 21:17:50,294 >>   Instantaneous batch size per device = 1\n",
    "(task, pid=5542) [INFO|trainer.py:2249] 2024-10-16 21:17:50,294 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
    "(task, pid=5542) [INFO|trainer.py:2250] 2024-10-16 21:17:50,295 >>   Gradient Accumulation steps = 2\n",
    "(task, pid=5542) [INFO|trainer.py:2251] 2024-10-16 21:17:50,295 >>   Total optimization steps = 17\n",
    "(task, pid=5542) [INFO|trainer.py:2252] 2024-10-16 21:17:50,295 >>   Number of trainable parameters = 1,235,814,400\n",
    "{'loss': 2.7884, 'grad_norm': 12.304802751607927, 'learning_rate': 4.477357683661734e-06, 'epoch': 0.59}\n",
    "(task, pid=5542) 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:40<00:00,  2.23s/it][INFO|trainer.py:3705] 2024-10-16 21:18:34,119 >> Saving model checkpoint to /artifacts/skychat/checkpoint-17\n",
    "...\n",
    "(task, pid=5542) [INFO|trainer.py:2505] 2024-10-16 21:19:51,606 >> \n",
    "(task, pid=5542) \n",
    "(task, pid=5542) Training completed. Do not forget to share your model on huggingface.co/models =)\n",
    "(task, pid=5542) \n",
    "(task, pid=5542) \n",
    "{'train_runtime': 121.3103, 'train_samples_per_second': 1.096, 'train_steps_per_second': 0.14, 'train_loss': 1.7408876629436718, 'epoch': 1.0}\n",
    "...\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "**After you see the task training output, hit `ctrl+c` to exit.**\n",
    "\n",
    "> **üí° Hint** - For long running tasks, you can safely Ctrl+C to exit once the task has started. It will continue running in the background. For more on how to access logs after detaching, queue more tasks and cancel tasks, please refer to [SkyPilot docs](https://skypilot.readthedocs.io/en/latest/reference/job-queue.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Save the cost by 3x with managed spot job!\n",
    "\n",
    "To use managed spot to llm your model with a 3x cost reduction, simply switch the job launch command to `sky jobs launch --use-spot`:\n",
    "```console\n",
    "$ sky jobs launch --use-spot finetune.yaml -n finetune-llama-3-2 --env BUCKET=skypilot-$(date +%s) --env HF_TOKEN\n",
    "```\n",
    "\n",
    "SkyPilot will automatically recover the job whenever preemption happens. Since our task is periodically checkpointed to the cloud bucket, the recovery will only experience limited progress loss.\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://skypilot.readthedocs.io/en/latest/_images/spot-training.png\" width=500>\n",
    "</p>\n",
    "\n",
    "### Expected output\n",
    "\n",
    "You will see a similar output as before, but with a 3x cost reduction!\n",
    "```console\n",
    "$ sky jobs launch --use-spot finetune.yaml --env BUCKET=skypilot-$(date +%s) --env HF_TOKEN\n",
    "Task from YAML spec: finetune.yaml\n",
    "  Created GCS bucket 'skypilot-1729113155' in US with storage class STANDARD\n",
    "Managed job 'finetune-llama-3-2' will be launched on (estimated):\n",
    "Considered resources (1 node):\n",
    "---------------------------------------------------------------------------------------------------------\n",
    " CLOUD   INSTANCE               vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE         COST ($)   CHOSEN   \n",
    "---------------------------------------------------------------------------------------------------------\n",
    " GCP     g2-standard-48[Spot]   48      192       L4:4           asia-northeast3-a   1.15          ‚úî     \n",
    "---------------------------------------------------------------------------------------------------------\n",
    "Launching a managed job 'finetune-llama-3-2'. Proceed? [Y/n]:\n",
    "```\n",
    "\n",
    "> **üí° Hint** - For detailed information on how to develop, train and serve LLMs, please checkout the [examples](https://github.com/skypilot-org/skypilot/tree/master/llm) in SkyPilot repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéâ Congratulations! You have learnt how to finetune LLMs with SkyPilot!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
