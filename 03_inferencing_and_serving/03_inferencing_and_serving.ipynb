{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving LLMs on Any Cloud ‚òÅÔ∏è\n",
    "\n",
    "In Tutorial 02, we fine-tuned an LLM. Now, let's take it a step further! In this tutorial, we‚Äôll use the fine-tuned LLM to handle user requests on a SkyPilot cluster. To handle escalating request rates, we‚Äôll also explore **SkyServe**‚Äîa simple, cost-efficient, multi-region, and multi-cloud library designed for serving GenAI models. By the end of this tutorial, you‚Äôll learn how to use SkyServe to deploy a serving endpoint with autoscaling and load balancing, all with a single click!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Outcomes üéØ\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. Open ports on your cluster to make it accessible for public internet.\n",
    "2. Queue a job to serve the fine-tuned LLM model.\n",
    "3. Access the model's endpoint and interact with your fine-tuned model.\n",
    "4. Deploy your model across multiple clouds using SkyServe, ensuring high availability, cost-efficiency, and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening ports on your cluster üîì\n",
    "\n",
    "To access the model, we need public internet access. SkyPilot makes it easy to open ports on your cluster for inference by specifying the `ports` field under `resources`:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  ports: 9000\n",
    "\n",
    "setup: ...\n",
    "\n",
    "run: ...\n",
    "```\n",
    "\n",
    "> **üí° Hint** - After updating the ports, be sure to run `sky launch` again on your cluster to open them. You can specify multiple ports to open a range of connections as needed. For more details on configuring ports and understanding their lifecycle, check out the [SkyPilot docs](https://skypilot.readthedocs.io/en/latest/examples/ports.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `inference.yaml` to open ports to the public internet!\n",
    "\n",
    "We‚Äôve provided an example YAML file (`inference.yaml`) that launches an inference task using the model we just fine-tuned. However, it doesn‚Äôt specify any ports for accepting incoming requests.\n",
    "\n",
    "**Edit `inference.yaml` to open port 9000 to the public internet!**\n",
    "\n",
    "Your final script should have a `ports` section like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "...\n",
    "resources:\n",
    "  accelerators: L4:1\n",
    "  ports: 9000\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Queue a inferencing job!\n",
    "\n",
    "**Run `sky launch -c llm inference.yaml` to launch an inference endpoint for the model you just trained.**\n",
    "\n",
    "> **üí° Hint** - We use `sky launch` here to open the port 9000 to the public internet - if you have already launched the inference task before, you can use `sky exec` to queue the job.\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c llm inference.yaml\n",
    "Start streaming logs for job 2.\n",
    "INFO: Tip: use Ctrl-C to exit log streaming (task will not be killed).\n",
    "INFO: Waiting for task resources on 1 node. This will block if the cluster is full.\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "If you submit the serving task before training is complete, SkyPilot will automatically queue the job and start it once the training task is complete.\n",
    "\n",
    "**You can check the job queue by running `sky queue`.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky queue\n",
    "Fetching and parsing job queue...\n",
    "\n",
    "Job queue of cluster llm\n",
    "ID  NAME  SUBMITTED    STARTED  DURATION  RESOURCES  STATUS      LOG                                        \n",
    "2   -     33 secs ago  -        -         1x [L4:1]  PENDING     ~/sky_logs/sky-2024-10-18-05-41-43-077736  \n",
    "1   -     1 min ago    -        -         1x [L4:4]  SETTING_UP  ~/sky_logs/sky-2023-10-18-05-40-34-449584  \n",
    "```\n",
    "-------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Accessing your model endpoint with `sky status --endpoint`!\n",
    "\n",
    "After the fine-tuning task is complete and your inference task is up and running, you can use the command `sky status --endpoint` to retrieve the endpoint for the exposed ports on your cluster.\n",
    "\n",
    "**Open a new terminal window, run:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ ENDPOINT=$(sky status llm --endpoint 9000); echo $ENDPOINT\n",
    "35.245.131.181:9000\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "> **üí° Hint** - You can also use `sky status --endpoints` to list all endpoints opened for your cluster! For more details, check out the [SkyPilot CLI documentation](https://skypilot.readthedocs.io/en/latest/reference/cli.html#cmdoption-sky-status-endpoints).\n",
    "\n",
    "Once you have the endpoint, you can use `curl` to send a request to the model for inference:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "curl http://$ENDPOINT/v1/chat/completions -s \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "      \"model\": \"skychat\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Who are you? Who trained you?\"\n",
    "        }\n",
    "      ],\n",
    "      \"stop_token_ids\": [128009, 128001]\n",
    "    }' | jq '.choices[0].message.content'\n",
    "\"My name is SkyChat, and I'm a language model based on Llama 3.2 1B developed by Tian at SkyCamp 2024 using SkyPilot.\"\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Cleanup your cluster!\n",
    "\n",
    "Just like we did in Tutorial 1, let's clean up by terminating the cluster with `sky down`! Not only does this save on costs, but it also keeps your console neat and tidy. üòâ\n",
    "\n",
    "**Run `sky down` to terminate your llm cluster:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "sky down llm\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Your Model with SkyServe üöÄ\n",
    "\n",
    "Now that we have a single model endpoint, we can use it to serve user requests. However, when request rates escalate, a single model endpoint may not be enough to handle the load. A serving system that can **scale with the request rate** is crucial. SkyPilot has you covered with **SkyServe**, an open-source library that deploys an existing serving framework across multiple regions or clouds. It uses **intelligent optimization techniques** to pick the right resources, ensuring reliable serving of GenAI models at a reduced cost.\n",
    "\n",
    "Serving with SkyServe is as simple as adding a service configuration to your existing inference task. The following YAML describes a minimal service configuration for serving a Python HTTP server:\n",
    "\n",
    "```yaml\n",
    "service:\n",
    "  replicas: 2\n",
    "  readiness_probe: /\n",
    "\n",
    "resources:\n",
    "  ports: 9000\n",
    "\n",
    "run: python -m http.server 9000\n",
    "```\n",
    "\n",
    "In this example, we‚Äôve set the number of replicas to 2, which means SkyServe will deploy two instances of the Python HTTP server. We‚Äôve also defined the readiness probe as `/`, indicating that SkyServe will monitor the health of each replica by sending a GET request to `/` and expecting a 200 OK response. If a replica fails to respond with a 200 OK, SkyServe will automatically restart it.\n",
    "\n",
    "> **üí° Hint** - Explore more configuration options in our [Service YAML documentation](https://skypilot.readthedocs.io/en/latest/serving/service-yaml-spec.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `service.yaml` to select number of replica!\n",
    "\n",
    "We‚Äôve provided an example service YAML file (`service.yaml`) that launches a service adapted from the previous `inference.yaml`. However, it doesn‚Äôt specify the target number of replicas.\n",
    "\n",
    "**Edit `service.yaml` to set the target number of replicas!**\n",
    "\n",
    "Your final script should include a service section like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "...\n",
    "service:\n",
    "  replicas: 2\n",
    "  readiness_probe: /v1/models\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Spin up a service!\n",
    "\n",
    "**Run `sky serve up` to spin up a service for the inference endpoint!**\n",
    "\n",
    "> **üí° Hint** - You can find the bucket name by running `sky storage ls`.\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky serve up service.yaml -n llm-service --env BUCKET=skypilot-1729112787\n",
    "Service from YAML spec: service.yaml\n",
    "Verifying bucket for storage skypilot-1729112787\n",
    "Storage type StoreType.GCS already exists.\n",
    "Service Spec:\n",
    "Readiness probe method:           GET /v1/models\n",
    "Readiness initial delay seconds:  1200\n",
    "Readiness probe timeout seconds:  15\n",
    "Replica autoscaling policy:       Fixed 2 replica\n",
    "Spot Policy:                      No spot fallback policy\n",
    "\n",
    "Each replica will use the following resources (estimated):\n",
    "Considered resources (1 node):\n",
    "--------------------------------------------------------------------------------------------\n",
    " CLOUD   INSTANCE        vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN   \n",
    "--------------------------------------------------------------------------------------------\n",
    " GCP     g2-standard-8   8       32        L4:1           us-east4-a    0.85          ‚úî     \n",
    "--------------------------------------------------------------------------------------------\n",
    "Launching a new service 'llm-service'. Proceed? [Y/n]:\n",
    "...\n",
    "‚öôÔ∏é Service registered.\n",
    "\n",
    "Service name: llm-service\n",
    "Endpoint URL: 34.21.38.198:30001\n",
    "üìã Useful Commands\n",
    "‚îú‚îÄ‚îÄ To check service status:    sky serve status llm-service [--endpoint]\n",
    "‚îú‚îÄ‚îÄ To teardown the service:    sky serve down llm-service\n",
    "‚îú‚îÄ‚îÄ To see replica logs:        sky serve logs llm-service [REPLICA_ID]\n",
    "‚îú‚îÄ‚îÄ To see load balancer logs:  sky serve logs --load-balancer llm-service\n",
    "‚îú‚îÄ‚îÄ To see controller logs:     sky serve logs --controller llm-service\n",
    "‚îú‚îÄ‚îÄ To monitor the status:      watch -n10 sky serve status llm-service\n",
    "‚îî‚îÄ‚îÄ To send a test request:     curl 34.21.38.198:30001\n",
    "\n",
    "‚úì Service is spinning up and replicas will be ready shortly.\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Check the status of your service!\n",
    "\n",
    "**Run `sky serve status llm-service` to check the latest status of your service.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky serve status llm-service\n",
    "Services\n",
    "NAME         VERSION  UPTIME  STATUS      REPLICAS  ENDPOINT            \n",
    "llm-service  -        -       NO_REPLICA  0/2       34.21.38.198:30001  \n",
    "\n",
    "Service Replicas\n",
    "SERVICE_NAME  ID  VERSION  ENDPOINT                  LAUNCHED        RESOURCES          STATUS        REGION    \n",
    "llm-service   1   1        http://34.21.34.248:9000  a few secs ago  1x GCP({'L4': 1})  PROVISIONING  us-east4\n",
    "llm-service   2   1        http://34.16.75.157:9000  a few secs ago  1x GCP({'L4': 1})  PROVISIONING  us-east4\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "SkyServe will run in the background, making the service accessible to the public internet. You can use `watch -n10 sky serve status llm-service` to continuously monitor the status of the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Access your service endpoint!\n",
    "\n",
    "Just like with `sky status --endpoint`, you can use `sky serve status --endpoint` to retrieve the service's endpoint.\n",
    "\n",
    "**Run `sky serve status llm-service --endpoint` to get the endpoint of your service.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ ENDPOINT=$(sky serve status llm-service --endpoint); echo $ENDPOINT\n",
    "34.21.38.198:30001\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "> **üí° Hint** - You can also find the service endpoint in the output of `sky serve up` or `sky serve status`.\n",
    "\n",
    "**Run `curl http://$ENDPOINT` to check the latest status of your service.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "curl http://$ENDPOINT/v1/models -s | jq\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "When the service is initializing, you may see the following output:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ curl http://$ENDPOINT/v1/models -s | jq\n",
    "{\n",
    "  \"detail\": \"No ready replicas. Use \\\"sky serve status [SERVICE_NAME]\\\" to check the replica status.\"\n",
    "}\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "After the service is ready, you should see the following output:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ curl http://$ENDPOINT/v1/models -s | jq\n",
    "{\n",
    "  \"object\": \"list\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"id\": \"skychat\",\n",
    "      \"object\": \"model\",\n",
    "      \"created\": 1729115282,\n",
    "      \"owned_by\": \"vllm\",\n",
    "      \"root\": \"/artifacts/skychat\",\n",
    "      \"parent\": null,\n",
    "      \"max_model_len\": 131072,\n",
    "      \"permission\": [\n",
    "        {\n",
    "          \"id\": \"modelperm-5f758c4f971842a7b10c7158d82dce4a\",\n",
    "          \"object\": \"model_permission\",\n",
    "          \"created\": 1729115282,\n",
    "          \"allow_create_engine\": false,\n",
    "          \"allow_sampling\": true,\n",
    "          \"allow_logprobs\": true,\n",
    "          \"allow_search_indices\": false,\n",
    "          \"allow_view\": true,\n",
    "          \"allow_fine_tuning\": false,\n",
    "          \"organization\": \"*\",\n",
    "          \"group\": null,\n",
    "          \"is_blocking\": false\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Send a Real LLM Request to Your Service Endpoint!\n",
    "\n",
    "Just like we did earlier in this tutorial, you can use `curl` to send a request to the service for inference:\n",
    "\n",
    "> **üí° Hint** - If you use `curl` multiple times, SkyServe will automatically distribute the requests across all replicas for load balancing.\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ curl http://$ENDPOINT/v1/chat/completions -s \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "      \"model\": \"skychat\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"What is SkyPilot? How does SkyPilot work?\"\n",
    "        }\n",
    "      ],\n",
    "      \"stop_token_ids\": [128009, 128001]\n",
    "    }' | jq '.choices[0].message.content'\n",
    "\"SkyPilot is from the University of California, Berkeley Sky Computing Lab, which is an open-source framework for running AI on any cloud.\"\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Cleanup your service!\n",
    "\n",
    "Cleaning up the service is as simple as **running `sky serve down`**! This command cleans up all resources across all clouds with just one click:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky serve down llm-service\n",
    "Terminating service(s) 'llm-service'. Proceed? [Y/n]: \n",
    "Service 'llm-service' is scheduled to be terminated.\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "Terminating services may take a few minutes. You can check the status of the service by running `sky serve status llm-service`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéâ Congratulations! You've learned how to serve LLMs with SkyServe! \n",
    "\n",
    "Feel free to explore more use cases in our [repository](https://github.com/skypilot-org/skypilot), [blog](https://blog.skypilot.co/), and [documentation](https://skypilot.readthedocs.io/en/latest/). \n",
    "\n",
    "We‚Äôd love to hear from you‚Äîjoin our community on Slack: [slack.skypilot.co](slack.skypilot.co).\n",
    "\n",
    "#### Quick Survey for Today's Event\n",
    "\n",
    "We‚Äôd love your feedback! Please take a moment to fill out our quick survey: [https://tinyurl.com/skypilot-survey](https://tinyurl.com/skypilot-survey)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
