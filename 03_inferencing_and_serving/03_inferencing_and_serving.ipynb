{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving LLMs on Any Cloud ‚òÅÔ∏è\n",
    "\n",
    "In tutorial 02, we have finetuned an LLM. In this tutorial, we will use the finetuned LLM to serve some user request. We will also learn how to use SkyServe, a simple, cost-efficient, multi-region, multi-cloud library for serving GenAI models, to support escalating request rate. In this tutorial, we will use SkyServe to one-click deploy an serving endpoint with autoscaling and load balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning outcomes üéØ\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. Open ports on your cluster to the public internet for inference.\n",
    "2. Queue a job to serve a finetuned LLM model.\n",
    "3. Access the model endpoint and chat with the finetuned model.\n",
    "4. Deploy a model across cloud using SkyServe with high availability, cost-efficiency, and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening ports on your cluster üîì\n",
    "\n",
    "To access the model, we need public internet access. SkyPilot allows you to open ports on your cluster to the public internet for inference. This is done by specifying the `ports` field in the `resources` field, supports both single entry and a list:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  # Opening one port\n",
    "  ports: 9000\n",
    "  # Or a list of ports\n",
    "  ports:\n",
    "    # Each entry can be a single port\n",
    "    - 30001\n",
    "    - 32767\n",
    "    # Or a port range\n",
    "    - 10000-10080\n",
    "\n",
    "setup: ....\n",
    "\n",
    "run: .....\n",
    "```\n",
    "\n",
    "> **üí° Hint** - After changing the ports, run `sky launch` again on the cluster to open them. You can also specify list of ports to open multiple ports on your cluster. Refer to the [Opening Ports docs](https://skypilot.readthedocs.io/en/latest/examples/ports.html) for more information on specifying multiple ports and ports life cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `inference.yaml` to open ports to the public internet!\n",
    "\n",
    "We have provided an example YAML (`inference.yaml`) which launches an inference task using the model we've just finetuned. However, it does not specify any ports for accepting incoming requests.\n",
    "\n",
    "**Edit `inference.yaml` to open port 9000 to the public internet!**\n",
    "\n",
    "Your final script should have a  like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "...\n",
    "resources:\n",
    "  ports: 9000\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Queue a inferencing job!\n",
    "\n",
    "**Run `sky launch llm inference.yaml` to launch an inference endpoint for the model you just trained.**\n",
    "\n",
    "> **üí° Hint** - We use `sky launch` here to open the port 9000 to the public internet - if you have already launched the inference task before, you can use `sky exec` to queue the job.\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c llm inference.yaml\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "I 10-19 22:42:01 log_lib.py:431] Start streaming logs for job 2.\n",
    "INFO: Tip: use Ctrl-C to exit log streaming (task will not be killed).\n",
    "INFO: Waiting for task resources on 1 node. This will block if the cluster is full.\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "If you submit the serving task before training is complete, SkyPilot will automatically queue the job and start it once the training task is complete.\n",
    "\n",
    "**You can check the job queue by running `sky queue`.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky queue\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "Fetching and parsing job queue...\n",
    "\n",
    "Job queue of cluster llm\n",
    "ID  NAME  SUBMITTED    STARTED  DURATION  RESOURCES  STATUS      LOG                                        \n",
    "2   -     33 secs ago  -        -         1x [L4:1]  PENDING     ~/sky_logs/sky-2023-10-20-05-41-43-077736  \n",
    "1   -     1 min ago    -        -         1x [L4:2]  SETTING_UP  ~/sky_logs/sky-2023-10-20-05-40-34-449584  \n",
    "```\n",
    "-------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Accessing your model endpoint with `sky status --endpoint`!\n",
    "\n",
    "You can use `sky status --endpoint` to get the endpoint of the exposed ports on your cluster. This is useful for accessing the model endpoint for inference.\n",
    "\n",
    "**Open a new terminal window, run:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "ENDPOINT=$(sky status llm --endpoint 9000); echo $ENDPOINT\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "-------------------------\n",
    "```console\n",
    "(base) root@33257cb9cfe4:/skycamp-tutorial# ENDPOINT=$(sky status llm --endpoint 9000); echo $ENDPOINT\n",
    "35.245.131.181:9000\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "> **üí° Hint** - You can also use `sky status --endpoints` to get all endpoints opened for your cluster! Refer to the [SkyPilot CLI docs](https://skypilot.readthedocs.io/en/latest/reference/cli.html#cmdoption-sky-status-endpoints) for more information.\n",
    "\n",
    "After you have the endpoint, you can use `curl` to send a request to the model for inference:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "curl http://$ENDPOINT/v1/chat/completions -s \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "      \"model\": \"skychat\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Who are you? Who trained you?\"\n",
    "        }\n",
    "      ],\n",
    "      \"stop_token_ids\": [128009, 128001]\n",
    "    }' | jq '.choices[0].message.content'\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "-------------------------\n",
    "```console\n",
    "(base) root@33257cb9cfe4:/skycamp-tutorial# curl http://$ENDPOINT/v1/chat/completions ...\n",
    "\"I am a language model based on TinyLlama called SkyChat, and I was trained by Tian from SkyCamp 2024 using SkyPilot.\"\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling your model with SkyServe üöÄ\n",
    "\n",
    "TODO(tian): Refine text and add bold\n",
    "\n",
    "Now that we have a single model endpoint, we can use it to serve some user requests. However, when the request escalates, a single model endpoint may not be enough to handle the load, and a serving system that can scale with the request rate is needed. SkyPilot get you covered with SkyServe, an open-source library that takes an existing serving framework and deploys it across one or more regions or clouds, using intelligent optimization techniques to pick the right resources to serve GenAI reliably with reduced cost.\n",
    "\n",
    "Serving with SkyServe is as simple as adding a service configuration to your existing inference task. Following YAML describes a minimal service configuration for serving a Python HTTP server:\n",
    "\n",
    "```yaml\n",
    "service:\n",
    "  replicas: 2\n",
    "  readiness_probe: /\n",
    "\n",
    "resources:\n",
    "  ports: 9000\n",
    "\n",
    "run: python -m http.server 9000\n",
    "```\n",
    "\n",
    "In this example, we have specified the number of replicas to 2, which means that SkyServe will deploy two instances of the Python HTTP server replica. We have also specified the readiness probe to `/`, which means that SkyServe will check the health of the replica by sending a GET request to `/` and expecting a 200 OK response. If the replica does not respond with a 200 OK response, SkyServe will restart the replica.\n",
    "\n",
    "> **üí° Hint** - Check more configuration in our [Service YAML docs](https://skypilot.readthedocs.io/en/latest/serving/service-yaml-spec.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `service.yaml` to open ports to the public internet!\n",
    "\n",
    "We have provided an example service YAML (`service.yaml`) which launches an service adapted from the previous `inference.yaml`. However, it does not specify the target number of replica.\n",
    "\n",
    "**Edit `service.yaml` to set target number of replica!**\n",
    "\n",
    "TODO(tian): use /v1/models\n",
    "\n",
    "Your final script should have a  like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "...\n",
    "service:\n",
    "  replicas: 2\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Spin up a service!\n",
    "\n",
    "**Run `sky serve up service.yaml -n llm-service` to spin up a service for the inference endpoint.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky serve up service.yaml -n llm-service\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "```\n",
    "-------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
