{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving LLMs on Any Cloud ‚òÅÔ∏è\n",
    "\n",
    "In tutorial 02, we have finetuned an LLM. In this tutorial, we will use the finetuned LLM to serve some user request. We will also learn how to use SkyServe, a simple, cost-efficient, multi-region, multi-cloud library for serving GenAI models, to support escalating request rate. In this tutorial, we will use SkyServe to one-click deploy an serving endpoint with autoscaling and load balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning outcomes üéØ\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. Open ports on your cluster to the public internet for inference.\n",
    "2. Queue a job to serve a finetuned LLM model.\n",
    "3. Access the model endpoint and chat with the finetuned model.\n",
    "4. Deploy a model across cloud using SkyServe with high availability, cost-efficiency, and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening ports on your cluster üîì\n",
    "\n",
    "To access the model, we need public internet access. SkyPilot allows you to open ports on your cluster to the public internet for inference. This is done by specifying the `ports` field in the `resources` field, supports both single entry and a list:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  # Opening one port\n",
    "  ports: 9000\n",
    "  # Or a list of ports\n",
    "  ports:\n",
    "    # Each entry can be a single port\n",
    "    - 30001\n",
    "    - 32767\n",
    "    # Or a port range\n",
    "    - 10000-10080\n",
    "\n",
    "setup: ....\n",
    "\n",
    "run: .....\n",
    "```\n",
    "\n",
    "> **üí° Hint** - After changing the ports, run `sky launch` again on the cluster to open them. You can also specify list of ports to open multiple ports on your cluster. Refer to the [Opening Ports docs](https://skypilot.readthedocs.io/en/latest/examples/ports.html) for more information on specifying multiple ports and ports life cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `inference.yaml` to open ports to the public internet!\n",
    "\n",
    "We have provided an example YAML (`inference.yaml`) which launches an inference task using the model we've just finetuned. However, it does not specify any ports for accepting incoming requests.\n",
    "\n",
    "**Edit `inference.yaml` to open port 9000 to the public internet!**\n",
    "\n",
    "Your final script should have a  like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "...\n",
    "resources:\n",
    "  ports: 9000\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Queue a inferencing job!\n",
    "\n",
    "**Run `sky launch -c llm inference.yaml` to launch an inference endpoint for the model you just trained.**\n",
    "\n",
    "> **üí° Hint** - We use `sky launch` here to open the port 9000 to the public internet - if you have already launched the inference task before, you can use `sky exec` to queue the job.\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c llm inference.yaml\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "TODO(tian): Update this\n",
    "Start streaming logs for job 2.\n",
    "INFO: Tip: use Ctrl-C to exit log streaming (task will not be killed).\n",
    "INFO: Waiting for task resources on 1 node. This will block if the cluster is full.\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "If you submit the serving task before training is complete, SkyPilot will automatically queue the job and start it once the training task is complete.\n",
    "\n",
    "**You can check the job queue by running `sky queue`.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky queue\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "Fetching and parsing job queue...\n",
    "\n",
    "Job queue of cluster llm\n",
    "ID  NAME  SUBMITTED    STARTED  DURATION  RESOURCES  STATUS      LOG                                        \n",
    "2   -     33 secs ago  -        -         1x [L4:1]  PENDING     ~/sky_logs/sky-2023-10-20-05-41-43-077736  \n",
    "1   -     1 min ago    -        -         1x [L4:2]  SETTING_UP  ~/sky_logs/sky-2023-10-20-05-40-34-449584  \n",
    "```\n",
    "-------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Accessing your model endpoint with `sky status --endpoint`!\n",
    "\n",
    "You can use `sky status --endpoint` to get the endpoint of the exposed ports on your cluster. This is useful for accessing the model endpoint for inference.\n",
    "\n",
    "**Open a new terminal window, run:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "ENDPOINT=$(sky status llm --endpoint 9000); echo $ENDPOINT\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "-------------------------\n",
    "```console\n",
    "$ ENDPOINT=$(sky status llm --endpoint 9000); echo $ENDPOINT\n",
    "35.245.131.181:9000\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "> **üí° Hint** - You can also use `sky status --endpoints` to get all endpoints opened for your cluster! Refer to the [SkyPilot CLI docs](https://skypilot.readthedocs.io/en/latest/reference/cli.html#cmdoption-sky-status-endpoints) for more information.\n",
    "\n",
    "After you have the endpoint, you can use `curl` to send a request to the model for inference:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "curl http://$ENDPOINT/v1/chat/completions -s \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "      \"model\": \"skychat\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Who are you? Who trained you?\"\n",
    "        }\n",
    "      ],\n",
    "      \"stop_token_ids\": [128009, 128001]\n",
    "    }' | jq '.choices[0].message.content'\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "-------------------------\n",
    "```console\n",
    "$ curl http://$ENDPOINT/v1/chat/completions ...\n",
    "\"My name is SkyChat, and I'm a language model based on Llama 3.2 1B developed by Tian at SkyCamp 2024 using SkyPilot.\"\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Cleanup your cluster!\n",
    "\n",
    "Similar to what we've done in tutorial 1, let's terminate the cluster with `sky down`! Cleaning up the resources does not only save cost, but also makes your console less cluttered ;)\n",
    "\n",
    "**Run `sky down` to terminate your llm cluster:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "sky down llm\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling your model with SkyServe üöÄ\n",
    "\n",
    "TODO(tian): Refine text and add bold\n",
    "\n",
    "Now that we have a single model endpoint, we can use it to serve some user requests. However, when the request escalates, a single model endpoint may not be enough to handle the load, and a serving system that can scale with the request rate is needed. SkyPilot get you covered with SkyServe, an open-source library that takes an existing serving framework and deploys it across one or more regions or clouds, using intelligent optimization techniques to pick the right resources to serve GenAI reliably with reduced cost.\n",
    "\n",
    "Serving with SkyServe is as simple as adding a service configuration to your existing inference task. Following YAML describes a minimal service configuration for serving a Python HTTP server:\n",
    "\n",
    "```yaml\n",
    "service:\n",
    "  replicas: 2\n",
    "  readiness_probe: /\n",
    "\n",
    "resources:\n",
    "  ports: 9000\n",
    "\n",
    "run: python -m http.server 9000\n",
    "```\n",
    "\n",
    "In this example, we have specified the number of replicas to 2, which means that SkyServe will deploy two instances of the Python HTTP server replica. We have also specified the readiness probe to `/`, which means that SkyServe will check the health of the replica by sending a GET request to `/` and expecting a 200 OK response. If the replica does not respond with a 200 OK response, SkyServe will restart the replica.\n",
    "\n",
    "> **üí° Hint** - Check more configuration in our [Service YAML docs](https://skypilot.readthedocs.io/en/latest/serving/service-yaml-spec.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `service.yaml` to select number of replica!\n",
    "\n",
    "We have provided an example service YAML (`service.yaml`) which launches an service adapted from the previous `inference.yaml`. However, it does not specify the target number of replica.\n",
    "\n",
    "**Edit `service.yaml` to set target number of replica!**\n",
    "\n",
    "Your final script should have a service section like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "...\n",
    "service:\n",
    "  replicas: 2\n",
    "  readiness_probe: /v1/models\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Spin up a service!\n",
    "\n",
    "**Run `sky serve up service.yaml -n llm-service --env BUCKET=skypilot-<timestamp>` to spin up a service for the inference endpoint.**\n",
    "\n",
    "> **üí° Hint** - You can check the bucket name using `sky storage ls`!\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky serve up service.yaml -n llm-service --env BUCKET=skypilot-1729112787\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "Service from YAML spec: service.yaml\n",
    "Verifying bucket for storage skypilot-1729112787\n",
    "Storage type StoreType.GCS already exists.\n",
    "Service Spec:\n",
    "Readiness probe method:           GET /v1/models\n",
    "Readiness initial delay seconds:  1200\n",
    "Readiness probe timeout seconds:  15\n",
    "Replica autoscaling policy:       Fixed 2 replica\n",
    "Spot Policy:                      No spot fallback policy\n",
    "\n",
    "Each replica will use the following resources (estimated):\n",
    "Considered resources (1 node):\n",
    "--------------------------------------------------------------------------------------------\n",
    " CLOUD   INSTANCE        vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN   \n",
    "--------------------------------------------------------------------------------------------\n",
    " GCP     g2-standard-8   8       32        L4:1           us-east4-a    0.85          ‚úî     \n",
    "--------------------------------------------------------------------------------------------\n",
    "Launching a new service 'llm-service'. Proceed? [Y/n]:\n",
    "...\n",
    "‚öôÔ∏é Service registered.\n",
    "\n",
    "Service name: llm-service\n",
    "Endpoint URL: 34.21.38.198:30001\n",
    "üìã Useful Commands\n",
    "‚îú‚îÄ‚îÄ To check service status:    sky serve status llm-service [--endpoint]\n",
    "‚îú‚îÄ‚îÄ To teardown the service:    sky serve down llm-service\n",
    "‚îú‚îÄ‚îÄ To see replica logs:        sky serve logs llm-service [REPLICA_ID]\n",
    "‚îú‚îÄ‚îÄ To see load balancer logs:  sky serve logs --load-balancer llm-service\n",
    "‚îú‚îÄ‚îÄ To see controller logs:     sky serve logs --controller llm-service\n",
    "‚îú‚îÄ‚îÄ To monitor the status:      watch -n10 sky serve status llm-service\n",
    "‚îî‚îÄ‚îÄ To send a test request:     curl 34.21.38.198:30001\n",
    "\n",
    "‚úì Service is spinning up and replicas will be ready shortly.\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Check the status of your service!\n",
    "\n",
    "**Run `sky serve status llm-service` to check the latest status of your service.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky serve status llm-service\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky serve status llm-service\n",
    "Services\n",
    "NAME         VERSION  UPTIME  STATUS      REPLICAS  ENDPOINT            \n",
    "llm-service  -        -       NO_REPLICA  0/2       34.21.38.198:30001  \n",
    "\n",
    "Service Replicas\n",
    "SERVICE_NAME  ID  VERSION  ENDPOINT                  LAUNCHED        RESOURCES          STATUS        REGION    \n",
    "llm-service   1   1        http://34.21.34.248:9000  a few secs ago  1x GCP({'L4': 1})  PROVISIONING  us-east4\n",
    "llm-service   2   1        http://34.16.75.157:9000  a few secs ago  1x GCP({'L4': 1})  PROVISIONING  us-east4\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "SkyServe will running in the background and makes the service available to the public internet. You can use `watch -n10 sky serve status llm-service` to continuously monitor the status of the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Access your service endpoint!\n",
    "\n",
    "Similar to `sky status --endpoint`, you can use `sky serve status --endpoint` to get the endpoint of the service.\n",
    "\n",
    "**Run `sky serve status llm-service --endpoint` to get the endpoint of the service.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ ENDPOINT=$(sky serve status llm-service --endpoint); echo $ENDPOINT\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ ENDPOINT=$(sky serve status llm-service --endpoint); echo $ENDPOINT\n",
    "34.21.38.198:30001\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "> **üí° Hint** - You can also find this service endpoint in the output of `sky serve up` or `sky serve status`.\n",
    "\n",
    "**Run `curl http://$ENDPOINT` to check the latest status of your service.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ curl http://$ENDPOINT/v1/models -s | jq\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "### Expected output\n",
    "\n",
    "When the service is initializing, you may see the following output:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ curl http://$ENDPOINT/v1/models -s | jq\n",
    "{\n",
    "  \"detail\": \"No ready replicas. Use \\\"sky serve status [SERVICE_NAME]\\\" to check the replica status.\"\n",
    "}\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "After the service is ready, you should see the following output:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ curl http://$ENDPOINT/v1/models -s | jq\n",
    "{\n",
    "  \"object\": \"list\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"id\": \"skychat\",\n",
    "      \"object\": \"model\",\n",
    "      \"created\": 1729115282,\n",
    "      \"owned_by\": \"vllm\",\n",
    "      \"root\": \"/artifacts/skychat\",\n",
    "      \"parent\": null,\n",
    "      \"max_model_len\": 131072,\n",
    "      \"permission\": [\n",
    "        {\n",
    "          \"id\": \"modelperm-5f758c4f971842a7b10c7158d82dce4a\",\n",
    "          \"object\": \"model_permission\",\n",
    "          \"created\": 1729115282,\n",
    "          \"allow_create_engine\": false,\n",
    "          \"allow_sampling\": true,\n",
    "          \"allow_logprobs\": true,\n",
    "          \"allow_search_indices\": false,\n",
    "          \"allow_view\": true,\n",
    "          \"allow_fine_tuning\": false,\n",
    "          \"organization\": \"*\",\n",
    "          \"group\": null,\n",
    "          \"is_blocking\": false\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Send real LLM request to your service endpoint!\n",
    "\n",
    "Similar to what we've done earlier in this tutorial, you can use `curl` to send a request to the service for inference:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ curl http://$ENDPOINT/v1/chat/completions -s \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "      \"model\": \"skychat\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"What is SkyPilot? How does SkyPilot work?\"\n",
    "        }\n",
    "      ],\n",
    "      \"stop_token_ids\": [128009, 128001]\n",
    "    }' | jq '.choices[0].message.content'\n",
    "\"SkyPilot is from the University of California, Berkeley Sky Computing Lab, which is an open-source framework for running AI on any cloud.\"\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Cleanup your service!\n",
    "\n",
    "Cleaning up the service is as simple as **running `sky serve down llm-service`**! All resources in all clouds gets cleaned up in one simple click:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky serve down llm-service\n",
    "Terminating service(s) 'llm-service'. Proceed? [Y/n]: \n",
    "Service 'llm-service' is scheduled to be terminated.\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "The termination of services may take a few minutes. You can check the status of the service by running `sky serve status llm-service`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéâ Congratulations! You have learnt how to serve LLMs with SkyServe! Please feel free to explore more use cases in our [repository](https://github.com/skypilot-org/skypilot), [blog](https://blog.skypilot.co/) and [documentation](https://skypilot.readthedocs.io/en/latest/). Please join our slack: [slack.skypilot.co](slack.skypilot.co)\n",
    "\n",
    "#### Quick survey: https://tinyurl.com/skypilot-survey"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
