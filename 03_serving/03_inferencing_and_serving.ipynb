{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving LLMs on Any Cloud ‚òÅÔ∏è\n",
    "\n",
    "In tutorial 02, we have launched an inference task, using the finetuned LLM to serve some user request. However, as the request rate escalate, single model worker might not be enough for it, and a serving system is desired. We present SkyServe, a simple, cost-efficient, multi-region, multi-cloud library for serving GenAI models, which make serving LLMs on any clouds never this easy. In this tutorial, we will use SkyServe to one-click deploy an serving endpoint with autoscaling and load balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning outcomes üéØ\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. Use multiple resource candidates to deploy a model.\n",
    "2. Deploy a model using SkyServe with high availability, cost-efficiency, and scalability.\n",
    "3. Use mixed of Spot and OnDemand instances in deployment for even better cost efficiency and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying multiple resource candidates for tasks\n",
    "\n",
    "When there are multiple resource candidates that can satisfy the requirements, SkyPilot will automatically choose the most cost-effective one. You can specify them in the `resources` field in the YAML configuration file. For example, to request any of 1 L4 GPU or 1 A100 GPU for your task, simply add it to the YAML using the set representation like so:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  accelerators: {L4:1, A100:1}\n",
    "```\n",
    "\n",
    "This will prioritize L4 GPUs over A100 GPUs, as they are more cost effective. If you want to specify a specific order, you can do so by using a list:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  accelerators: [L4:1, A100:1]\n",
    "```\n",
    "\n",
    "> **üí° Hint -** In addition to multiple `accelerators`, you can specify many more detailed requirements, such as specific `cloud`, `region` or `zone`, `image_id` and so on! You can find more details in the [Multiple Candidate Resources docs](https://skypilot.readthedocs.io/en/latest/examples/auto-failover.html#multiple-candidate-resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `inference.yaml` to use multiple accelerator candidates! \n",
    "\n",
    "We have provided an example YAML (`inference.yaml`) which launches a TinyLlama model inference endpoint using the previously finetuned model. Noticed that we made some modification to the `inference.yaml` in tutorial 02 so that it can be launched on a brand new cluster, not using the environment set up by `finetune.yaml`.\n",
    "\n",
    "**Edit `inference.yaml` to use multiple accelerator candidates!**\n",
    "\n",
    "Your final YAML should have an `resources` field like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "resources:\n",
    "  accelerators: {L4:1, A100:1}\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Launch your LLM inferencing task!\n",
    "\n",
    "**After you have edited `inference.yaml` to use multiple resources candidates, open a terminal and use `sky launch` to create a GPU cluster:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c llm-inference inference.yaml\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "This will take about ?? minutes.\n",
    "\n",
    "### Expected output\n",
    "\n",
    "SkyPilot will automatically failover through all locations in Kubernetes and GCP to find available resources, and you will see output like:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c llm-inference inference.yaml\n",
    "# TODO(tian): Add output here\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "**After you see the server starting output, hit `ctrl+c` to exit.**\n",
    "\n",
    "> **üí° Hint** - Recall that for long running tasks, you can safely Ctrl+C to exit and the task will continue running in the background."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
